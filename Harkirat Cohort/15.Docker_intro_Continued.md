# Docker

Docker/containers are important for a few reasons - 
- Kubernetes/Container orchestration
- Running processes in isolated environments
- Starting projects/auxilary services locally

Containers are a way to package and distribute software applications in a way that makes them easy to deploy and run consistently across different environments. They allow you to package an application, along with all its dependencies and libraries, into a single unit that can be run on any machine with a container runtime, such as Docker. It:
- Let you describe your configuration in a single file
- Can run in isolated environments
- Makes Local setup of OS projects a breeze
- Makes installing auxiliary services/DBs 

For Reference, the following command starts mongo in all operating systems- 
"docker run -d -p 27017:27017 mongo"

1. Docker Engine
Docker Engine is an open-source containerization technology that allows developers to package applications into container.
Containers are standardized executable components combining application source code with the operating system (OS) libraries and dependencies required to run that code in any environment.

2. Docker CLI
The command line interface lets you talk to the docker engine and lets you start/stop/list containers.

3. Docker registry
Its a place where people stored popular codebases. It is similar to github, but it lets you push images rather than sourcecode.

Docker engine is the actual software of docker thats running in our machine.
The terminal when runs "docker run mongo" it fetches the mongo image from docker registry, if its not there in our machine.

### Images vs Containers
A Docker image is a lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, environment variables, and config files. A good mental model for an image is Your codebase on github
 
A container is a running instance of an image. It encapsulates the application or service and its dependencies, running in an isolated environment.
A good mental model for a container is when you run node index.js on your machine from some source code you got from github.

Docker engine has deamon(means something that listening on events). You can talk to daemon by docker desktop gui, cli, REST api call(can send req to socket_url which can be converted to an http url)...

**Port Mapping:**
Browser have all set of ports, similarly docker container has all sets of ports. We have to map port of our pc to docker so that req coming at some port of our pc maps to the docker port.
"docker run -d -p 27018:27017 mongo"



### Common docker commands
1. `docker images`
Shows you all the images that you have on your machine
2. `docker ps`
Shows you all the containers you are running on your machine
3. `docker run`
Lets you start a container
-p ⇒ let’s you create a port mapping
-d. ⇒ Let’s you run it in detatched mode
4. `docker build`
Lets you build an image.
5. `docker push`
Lets you push your image to a registry
6. Extra commands
`docker kill` - followed by the container id to kill a container
`docker exec`
7. `docker rmi mongo --force`
Lets you remove a image (her e mongo) form your machine.


## Dockerfile
If you want to create an image from your own code, that you can push to dockerhub, you need to create a Dockerfile for your application. A Dockerfile is a text document that contains all the commands a user could call on the command line to create an image.
Lets say there are 5 commands you need to run one after the other to start a nodejs project, you write the same commands in a docker file that will be able to create a single image that people can run on their machine.

How to write a dockerfile:
A dockerfile has 2 parts
- Base image
- Bunch of commands that you run on the base image (to install dependencies like Node.js)

Create a root level .dockerfile, in that you write this kinda code:

Eg:
```dockerfile
FROM node:20

WORKDIR /app

COPY . .

RUN npm install
RUN npx prisma generate
RUN npm run build

EXPOSE 3000

CMD ["node", "dist/index.js"]
```

**Common commands**
- The first thing you have to do whenever youre defining a dockerfile is define whats called a base image, from where it will start.
- Then there is working directory path.
- COPY . . means if we run docker build then that copy means that copy everything from source folder of that application over to the working directory /app.
- You will make sure you put node modules in the .dockerignore file so that it dont copy the whole node module there.
- Then you run all the commands to run that image.
- Then we expose the port.
- All this runs when you create the image.
- Then CMD is written, which will run after we start the container. So we write CMD on things that actually start your application, and you write RUN on things that bootstrap your application.

- `FROM`: Its a base image, people has already made some base image, which has node, npm, nvm already installed in that base image.
- `WORKDIR`: This is going to be your working directory for all the commands that are going to come. Sets the working directory for any `RUN`, `CMD`, `ENTRYPOINT`, `COPY` instructions that follow it.
- `RUN`: Executes any commands in a new layer on top of the current image and commits the results.
- `CMD`: Provides defaults for executing a container. There can only be one CMD instruction in a Dockerfile.
- `EXPOSE`: Informs Docker that the container listens on the specified network ports at runtime.
- `ENV`: Sets the environment variable.
- `COPY`: Allow files from the Docker host to be added to the Docker image
>
Add a .dockerignore so that node_modules don’t get copied over 
>

**Building images**
Now that you have a dockerfile in your project, try building a docker image from it
`docker build -t image_name .`
- This . at the end tells that make the image of the current directory.
Now if you try to look at your images, you should notice a new image created
`docker images`
Now you can try and run that image you created in your machine
`docker run -p 3000:3000 image_name`

**Passing in the env variables:**
The env variables we put at the .env file. And we dont push it at the github, so as we dont push it the docker image too.
The env which you think you can push you can write like ENV env_name=value in the dockerfile itself.

`docker run -p 3000:3000 -e DATABASE_URL="postgres://xxxxx" image_name`
The -e argument let’s you send in environment variables to your node.js app.
You dont hard code the env variables to the docker file, as it can gt leaked. So when you start an image is when you should inject an env variable.


### docker exec
It is a command to execute a command inside a container. Lets say you want to ssh into that container then you use this command. Or lets say you want to access/see the file system inside that container.
`docker exec -it <container_name_or_id> /bin/bash`
if you wanted to run the command interactively...

>
Docker container has its own network, its own private IP.
>


## Layers in Docker
Its a important concept to know in docker specially if you are optimizing docker files.
Layers are a fundamental part of the image architecture that allows Docker to be efficient, fast, and portable.
Lets say there a 2 docker images and starting 3 lines are exactly the same, the three layers can be shared between the docker images.
A layer is changed then we no longer can use the cache, the subsequent next layers on top will also change.
How layers are made - 
1. Base Layer: The starting point of an image, typically an operating system (OS) like Ubuntu, Alpine, or any other base image specified in a Dockerfile.
2. Instruction Layers: Each command in a Dockerfile creates a new layer in the image. These include instructions like `RUN`, `COPY`, which modify the filesystem by installing packages, copying files from the host to the container, or making other changes. Each of these modifications creates a new layer on top of the base layer.
3. Reusable & Shareable: Layers are cached and reusable across different images, which makes building and sharing images more efficient. If multiple images are built from the same base image or share common instructions, they can reuse the same layers, reducing storage space and speeding up image downloads and builds.
4. Immutable: Once a layer is created, it cannot be changed. If a change is made, Docker creates a new layer that captures the difference. This immutability is key to Docker's reliability and performance, as unchanged layers can be shared across images and containers.
>
See if a layer is cached then every layer before it is cached, if a layer becomes uncached, then every layer after it also becomes uncached.
>
When you build a docker file:
- Base image creates the first layer
- Each RUN, COPY , WORKDIR  command creates a new layer
- Layers can get re-used across docker builds (notice CACHED in 1/6), so some of the layers will be cached and reused.
- Also layers can get reused accross different images too.
- If you change your Dockerfile, layers can get re-used based on where the change was made
    - so if you have a use case with building website, you want the number of cached layers to be high. As youre rebuilding projects, you want as many things as they can get cached. So thats something you optimize for. So whenever you write your docker file you write it in a way such that accross rebuilds, the number of cached layers is much higher.

How often in a project do you think dependencies change?
How often does the npm install layer need to change?
Wouldn’t it be nice if we could cache the npm install step considering dependencies don’t change often?

We want as many layers cached as possible. 
Case 1 - You change your source code (but nothing in package.json/prisma) then the npm install and prisma commands are cached, they aint re run.
        See we can copy `COPY package* .` and `COPY ./prisma .` and then run `RUN npm install` and `RUN npx prisma generate`, and then `COPY . .` so that if there is no change in dependencies then its cached.
Case 2 - You change the package.json file (added a dependency) then npm commands and copy commands are not cached and they run.

Remember, the thing that changes the most that you want to add at very end so that most layers can be cached.

- See lets say you have two projects to do docker build, in both there are docker files and both have `FROM node:20`. And rest content is not same lets say. Now when you do one docker build then it will go to dockerhub and the imgage of node:20 gets installed. It will store that image somewhere. Then the docker imgae of that project created. Now when you docker build other project then the node:20 image wont be getting from dockerhub rather, as its already present in the machine itself (its a cached image), so it will get from the cached image itself. Now lets say the second line too same `WORKDIR /app`, then whatever image is generated then somehow the first two steps gets cached, so that the other project can use the same image. So its like two layers one of first line and when second line run a layer created after top of first layer and eventually more layers is added and the image is made. But if the first two layers if its used in 10 different docker files then all those docker files can get the cached layers/image.


## Networks and Volumes
When you have multiple containers running, then you need to allow containers to talk to each other. And lets say you run docker command to make a postgres container then you need to persist data accross restarts.

## Volumes
If you restart a mongo docker container, you will notice that your data goes away. 
This is because docker containers are transitory (they don’t retain data across restarts)
**Without volumes:**
- Start a mongo container locally
"docker run -p 27017:27017 -d mongo"
- Open it in MongoDB Compass and add some data to it
- Kill the container
"docker kill <container_id>"
- Restart the container
"docker run -p 27017:27017 -d mongo"
- Try to explore the database in Compass and check if the data has persisted (it wouldn’t)


**With Volumes:**
- Create a volume
"docker volume create volume_database"
- Mount the folder in mongo which actually stores the data to this volume
"docker run -v volume_database:/data/db -p 27017:27017 mongo"
the /data/db, this is the folder where mongodb actually stores the data.
- Open it in MongoDB Compass and add some data to it
- Kill the container
"docker kill <container_id>"
- Restart the container
"docker run -v volume_database:/data/db -p 27017:27017 mongo"
- Try to explore the database in Compass and check if the data has persisted (it will!)

>
So what happens is the mongodb container mounts this /data/db folder to a volume weve created, even if it goes down as long as youve restarted with this volume mounted to this folder, the data persists.
>

## Network
Lets say a docker container, one running a nodejs app and one running a mongo app, both have their set of ports active. Now the mongo db has a port mapping 27017 to the pc, like the pc coming to this 27017 will forward the req to the container. But we want the nodejs to connect to the mongo container, if you req 27017 to this container then local host for this container is the same container so it tries to find something running on 27017 on that container which nothing is running over there. As these are mini machines. And the mongodb is connected to the separate container of the host machine. So we need to connect both of them, like we need to make sure they are part of same network. 
In Docker, a network is a powerful feature that allows containers to communicate with each other and with the outside world.
Docker containers can’t talk to each other by default. Like a docker container of mongo and a docker container of express, when you want them to talk to each other then they need to be a part of a common network.
localhost on a docker container means it's own network and not the network of the host machine
Means you created a mongodb container and a node container, now nodejs container has its own set of ports and mongodb has its own set of ports. now to make sure to connect the nodejs port 27017 to the mongodb 27017 port then you will need network.

- Clone the repo - https://github.com/100xdevs-cohort-2/week-15-live-2.2
- Build the image
`docker build -t image_tag .`
- Create a network
`docker network create my_custom_network`
- Start the backend process with the network attached to it
`docker run -d -p 3000:3000 --name backend --network my_custom_network image_tag`
- Start mongo on the same network
`docker run -d -v volume_database:/data/db --name mongovarun --network my_custom_network -p 27017:27017 mongo`
So you dont need to add the port mapping as the containers are talking to each other then you dont need to expose the port to the host machine.
- So in your app db file write the connection string as "mongodb://mongovarun:27017/myDatabase"
- Check the logs to ensure the db connection is successful
`docker logs <container_id>`
- Try to visit an endpoint and ensure you are able to talk to the database
- If you want, you can remove the port mapping for mongo since you don't necessarily need it exposed on your machine

**Types of networks:**
- Bridge: The default network driver for containers. When you run a container without specifying a network, it's attached to a bridge network. It provides a private internal network on the host machine, and containers on the same bridge network can communicate with each other.
- Host: Removes network isolation between the container and the Docker host, and uses the host's networking directly. This is useful for services that need to handle lots of traffic or need to expose many ports.



`docker kill <container_id>` command kills the container.
`docker rmi  <image_name>` command kills the image. if some error then write --force flag next
`docekr exec -it <container_name_or_id>` command lets you in the cli of the container. -it means run it interactively.



### Pushing to dockerhub
- Once you've created your image, you can push it to dockerhub to share it with the world.
- Signup to dockerhub
- Create a new repository
- Login to docker cli
    - docker login
    - you might have to create an access token - https://docs.docker.com/security/for-developers/access-tokens/
- Push to the repository
`docker push your_username/your_reponame:tagname`


If you have login in docker with google or github then you wont be having password, so in that case in dockerhub got to profile then security and create an access token so that you can login in to dockerhub through your terminal.
While building docker image after writing name after -t if you write :tag_name then it will gave a tag name to your docker image which you will see in dockerhub when you pusi it.

If you have real CI/CD, on every commit youre pushing an image to dockerhub with tag name as that small hash code(thats a standard practice) when youre sending your codebase with the image created to dockerhub and then eventually pulling it from dockerhub in your Kubernetes cluster, this is like a common pattern with a lot of big companies.
See there is github where your project is there, on every commit on github, it will spawn an ubuntu container which will create an image and push that imge on dockerhub, and then your Kubernetes cluster running on aws which has a bunch of small machines will be pulling the latest tag what you might have pushed over to the dockerhub. So thats the benefit of using tags.


## Docker Compose
1. When youre creating your own project you should use docker compose
2. When youre setting up a bunch of projects maintainers of those projects use docker compose so you need to understand how it works.

A docker compose is a tool designed to help you define and run multi-container Docker applications. With compose, you use a YAML file to configure your application's services, networks and volumes. Then, with a single command you can create and start all the services from your configuration.
Lets say we have a full stack application with nodejs and mongo, so to run that app in container we have to connect mongo to the volume and the node container and mongo container both to the network only then we will be able to run the fullstack app.
If you dont have docker compose in multi-container docker application then: 
- Create a network
`docker network create my_custom_network`
- Create a volume
`docker volume create volume_database`
- Start mongo container
`docker run -d -v volume_database:/data/db --name mongo --network my_custom_network  mongo`
- Start backend container
If not build then build the image first.
`docker run -d -p 3000:3000 --name backend --network my_custom_network backend`

Now what if you give a single command that will bootstrap everything. That would start the container, create volume, create network, everyting...
Docker Compose is a tool designed to help you define and run multi-container Docker applications. With Compose, you use a YAML file to configure your application's services, networks, and volumes. Then, with a single command, you can create and start all the services from your configuration.

- Install docker-compose - https://docs.docker.com/compose/install/
- Create a yaml file describing all your containers and volumes
- Whenever you have multiple services connected to the same docker compose, they are by default attached by a network, docker compose creates a network for them. So they are all going to be in a same network.

```yaml
    # version of docker compose
    version: '3.8'                  
    services:
        # mongo service
        mongodb:                    
            image: mongo
            container_name: mongodb
            ports:
                - "27017:27017"
            # this attaches the volume to mongodb, mongodb_data points do /data/db
            volumes:
                - mongodb_data:/data/db
        # so this converts this to command "docker run --name mongodb -p 27017:27017 -v mongodb_data:/data/db mongo"

        # backend service
        backend22:
            image: backend
            container_name: backend_app
            # depends on here means only when the mongodb image actually starts after that it should start the backend container
            depends_on:
                - mongodb
            ports:
                - "3000:3000"
        # environment is an object its not an array
            environment:
                MONGO_URL: "mongodb://mongodb:27017"

    # this creates the volume
    volumes:
        mongodb_data:
```
image: backend in this plce we can do is build: . which means it builds the docker image first, rather than it takes the already built image.

file without the comments:
```yaml
    version: '3.8'                  
    services:
        mongodb:                    
            image: mongo
            container_name: mongodb
            ports:
                - "27017:27017"
            volumes:
                - mongodb_data:/data/db

        backend22:
            image: backend
            container_name: backend_app
            depends_on:
                - mongodb
            ports:
                - "3000:3000"
            environment:
                MONGO_URL: "mongodb://mongodb:27017"

    volumes:
        mongodb_data:
```



see the equivalent json file looks like this:
```json
{
    "version": "3.8",
    "services": {
        "mongodb": {
            "image": "mongo",
            "container_name": "mongodb",
            "ports": [
                "27017:27017"
            ],
            "volumes": [
                "mongodb_data:/data/db"
            ]
        }
    },
    "backend22": {
        "image": "backend",
        "container_name": "backend_app",
        "depends_on": [
            "mongodb",
            "kafka"
        ],
        "ports": [
            "3000:3000"
        ],
        "environment": {
            "MONGO_URL": "mongodb://mongodb:27017"
        }
    },
    "volumes": {
        "mongodb_data": null
    }
}
```


- Start the compose
`docker-compose up`
- Stop everything (including volumes)
`docker-compose down --volumes`

Now with this you have to run a single command now to start everything in the project locally. You dont need to start mongodb separately, you dont need to build the image separately, need not to run other commands, you run a single command "docker-compose up" and it starts everything, thats a benefit of using docker compose.



## Docker Bind Mount
One more concept that will become important as we're trying to containarize our nextjs, react or even nodejs app while running them locally is Bind Mounts. This was the way to do something like volumes and volumes were introduced.
When we use a bind mounts, a file or directory on the host machine is mounted into the container.
When creating a volume we have mount a folder /data/db to the volume we create until now. But lets say we want to bind a folder in the host machine i.e. our local pc to the folder in the container. Means when we make changes in the host folder it reflects the changes in the container folder and vice versa.
Now lets say we have made a container in which there is a next app. We made a dockerfile and build an image. Then we run that image in a container. Now when we change something in our code it wont reflect in the container running, as there wont be any hot reloading happening, obviousle, image is made with a copy of the directory.
Nextjs container we made has its own set of files. Changes made in local machine file wont reflect in the nextjs container file.

So when we make changes in the local machine code and make that changes reflect in the docker container, for that bind mounts is useful. It can take folder from your machine and bind that folder into the container's folder.

`docker run -p -v ./app:nextapp/app 3000:3000 nextapp`
./app:nextapp/app first written the folder dir from your pc and second written the folder dir to mount in the container's folder.
This is how we could give a real local developement experience to our end user because our end user need to have hot reloading, you cant expect your end user, they will start and then edit a file and stop, restart the docker container. So this is the way to do that.