# Docker

Docker/containers are important for a few reasons - 
- Kubernetes/Container orchestration
- Running processes in isolated environments
- Starting projects/auxilary services locally

Containers are a way to package and distribute software applications in a way that makes them easy to deploy and run consistently across different environments. They allow you to package an application, along with all its dependencies and libraries, into a single unit that can be run on any machine with a container runtime, such as Docker. It:
- Let you describe your configuration in a single file
- Can run in isolated environments
- Makes Local setup of OS projects a breeze
- Makes installing auxiliary services/DBs 

For Reference, the following command starts mongo in all operating systems- 
"docker run -d -p 27017:27017 mongo"

1. Docker Engine
Docker Engine is an open-source containerization technology that allows developers to package applications into container.
Containers are standardized executable components combining application source code with the operating system (OS) libraries and dependencies required to run that code in any environment.

2. Docker CLI
The command line interface lets you talk to the docker engine and lets you start/stop/list containers.

3. Docker registry
Its a place where people stored popular codebases. It is similar to github, but it lets you push images rather than sourcecode.

Docker engine is the actual software of docker thats running in our machine.
The terminal when runs "docker run mongo" it fetches the mongo image from docker registry, if its not there in our machine.

### Images vs Containers
A Docker image is a lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, environment variables, and config files. A good mental model for an image is Your codebase on github
 
A container is a running instance of an image. It encapsulates the application or service and its dependencies, running in an isolated environment.
A good mental model for a container is when you run node index.js on your machine from some source code you got from github.

Docker engine has deamon(means something that listening on events). You can talk to daemon by docker desktop gui, cli, REST api call(can send req to socket_url which can be converted to an http url)...

**Port Mapping:**
Browser have all set of ports, similarly docker container has all sets of ports. We have to map port of our pc to docker so that req coming at some port of our pc maps to the docker port.
"docker run -d -p 27018:27017 mongo"



### Common docker commands
1. docker images
Shows you all the images that you have on your machine
2. docker ps
Shows you all the containers you are running on your machine
3. docker run
Lets you start a container
-p ⇒ let’s you create a port mapping
-d. ⇒ Let’s you run it in detatched mode
4. docker build
Lets you build an image.
5. docker push
Lets you push your image to a registry
6. Extra commands
"docker kill" - followed by the container id to kill a container
"docker exec"
7. docker rmi mongo --force
Lets you remove a image (here mongo) form your machine.


## Dockerfile
If you want to create an image from your own code, that you can push to dockerhub, you need to create a Dockerfile for your application. A Dockerfile is a text document that contains all the commands a user could call on the command line to create an image.
Lets say there are 5 commands you need to run one after the other to start a nodejs project, you write the same commands in a docker file that will be able to create a single image that people can run on their machine.

How to write a dockerfile:
A dockerfile has 2 parts
- Base image
- Bunch of commands that you run on the base image (to install dependencies like Node.js)

Eg:
```dockerfile
FROM node:20

WORKDIR /app

COPY . .

RUN npm install
RUN npx prisma generate
RUN npm run build

EXPOSE 3000

CMD ["node", "dist/index.js"]
```

**Common commands**
`WORKDIR`: Sets the working directory for any `RUN`, `CMD`, `ENTRYPOINT`, `COPY` instructions that follow it.
`RUN`: Executes any commands in a new layer on top of the current image and commits the results.
`CMD`: Provides defaults for executing a container. There can only be one CMD instruction in a Dockerfile.
`EXPOSE`: Informs Docker that the container listens on the specified network ports at runtime.
`ENV`: Sets the environment variable.
`COPY`: Allow files from the Docker host to be added to the Docker image
>
Add a .dockerignore so that node_modules don’t get copied over 
>

**Building images**
Now that you have a dockerfile in your project, try building a docker image from it
"docker build -t image_name ."
 
Now if you try to look at your images, you should notice a new image created
"docker images"
Now you can try and run that image you created in your machine
"docker run -p 3000:3000 image_name"

**Passing in the env variables:**
"docker run -p 3000:3000 -e DATABASE_URL="postgres://avnadmin:AVNS_EeDiMIdW-dNT4Ox9l1n@pg-35339ab4-harkirat-d1b9.a.aivencloud.com:25579/defaultdb?sslmode=require" image_name"
The -e argument let’s you send in environment variables to your node.js app.
You dont hard code the env variables to the docker file, as it can gt leaked. So when you start an image is when you should inject an env variable.


### docker exec
It is a command to execute a command inside a container.
"docker exec -it <container_name_or_id> /bin/bash"
if you wanted to run the command interactively...

>
Docker container has its own network, its own private IP.
>


## Layers in Docker
Layers are a fundamental part of the image architecture that allows Docker to be efficient, fast, and portable.
Lets say there a 2 docker images and starting 3 lines are exactly the same, the three layers can be shared between the docker images.
A layer is changed then we no longer can use the cache, the subsequent next layers on top will also change.
How layers are made - 
1. Base Layer: The starting point of an image, typically an operating system (OS) like Ubuntu, Alpine, or any other base image specified in a Dockerfile.
2. Instruction Layers: Each command in a Dockerfile creates a new layer in the image. These include instructions like `RUN`, `COPY`, which modify the filesystem by installing packages, copying files from the host to the container, or making other changes. Each of these modifications creates a new layer on top of the base layer.
3. Reusable & Shareable: Layers are cached and reusable across different images, which makes building and sharing images more efficient. If multiple images are built from the same base image or share common instructions, they can reuse the same layers, reducing storage space and speeding up image downloads and builds.
4. Immutable: Once a layer is created, it cannot be changed. If a change is made, Docker creates a new layer that captures the difference. This immutability is key to Docker's reliability and performance, as unchanged layers can be shared across images and containers.


When you build a docker file:
- Base image creates the first layer
- Each RUN, COPY , WORKDIR  command creates a new layer
- Layers can get re-used across docker builds (notice CACHED in 1/6), so some of the layers will be cached and reused.

- If you change your Dockerfile, layers can get re-used based on where the change was made
- If a layer changes, all subsequent layers also change

How often in a project do you think dependencies change?
How often does the npm install layer need to change?
Wouldn’t it be nice if we could cache the npm install step considering dependencies don’t change often?

We want as many layers cached as possible. 
Case 1 - You change your source code (but nothing in package.json/prisma) then the npm install and prisma commands are cached, they aint re run.
        See we can copy COPY package* . and COPY ./prisma . and then run RUN npm install and RUN npx prisma generate, so that if there is no change in dependencies then its cached.
Case 2 - You change the package.json file (added a dependency) then npm commands and copy commands are not cached and they run.

Remember, the thing that changes the most that you want to add at very end so that most layers can be cached.



## Networks and Volumes
When you have multiple containers running, then you need to allow containers to talk to each other. And lets say you run docker command to make a postgres container then you need to persist data accross restarts.

## Volumes
If you restart a mongo docker container, you will notice that your data goes away. 
This is because docker containers are transitory (they don’t retain data across restarts)
**Without volumes:**
- Start a mongo container locally
"docker run -p 27017:27017 -d mongo"
- Open it in MongoDB Compass and add some data to it
- Kill the container
"docker kill <container_id>"
- Restart the container
"docker run -p 27017:27017 -d mongo"
- Try to explore the database in Compass and check if the data has persisted (it wouldn’t)


**With Volumes:**
- Create a volume
"docker volume create volume_database"
- Mount the folder in mongo which actually stores the data to this volume
"docker run -v volume_database:/data/db -p 27017:27017 mongo"
- Open it in MongoDB Compass and add some data to it
- Kill the container
"docker kill <container_id>"
- Restart the container
"docker run -v volume_database:/data/db -p 27017:27017 mongo"
- Try to explore the database in Compass and check if the data has persisted (it will!)


## Network
In Docker, a network is a powerful feature that allows containers to communicate with each other and with the outside world.
Docker containers can’t talk to each other by default.
localhost on a docker container means it's own network and not the network of the host machine
Means you created a mongodb container and a node container, now nodejs container has its own set of ports and mongodb has its own set of ports. now to make sure to connect the nodejs port 27017 to the mongodb 27017 port then you will need network.

- Clone the repo - https://github.com/100xdevs-cohort-2/week-15-live-2.2
- Build the image
"docker build -t image_tag ."
- Create a network
"docker network create my_custom_network"
- Start the backend process with the network attached to it
"docker run -d -p 3000:3000 --name backend --network my_custom_network image_tag"
- Start mongo on the same network
"docker run -d -v volume_database:/data/db --name mongo --network my_custom_network -p 27017:27017 mongo"
- Also in your app db file write the connection string as 'mongodb://my_custom_network:27017/myDatabase'
- Check the logs to ensure the db connection is successful
"docker logs <container_id>"
- Try to visit an endpoint and ensure you are able to talk to the database
- If you want, you can remove the port mapping for mongo since you don’t necessarily need it exposed on your machine

**Types of networks:**
- Bridge: The default network driver for containers. When you run a container without specifying a network, it's attached to a bridge network. It provides a private internal network on the host machine, and containers on the same bridge network can communicate with each other.
- Host: Removes network isolation between the container and the Docker host, and uses the host's networking directly. This is useful for services that need to handle lots of traffic or need to expose many ports.



"docker kill <container_id>" command kills the container.
"docker rmi  <image_name>" command kills the image. if some error then write --force flag next
"docekr exec -it <container_name_or_id>" command lets you in the cli of the container. -it means run it interactively.



### Pushing to dockerhub
- Once you’ve created your image, you can push it to dockerhub to share it with the world.
- Signup to dockerhub
- Create a new repository
- Login to docker cli
    - docker login
    - you might have to create an access token - https://docs.docker.com/security/for-developers/access-tokens/
- Push to the repository
"docker push your_username/your_reponame:tagname"


If you have login in docker with google or github then you wont be having password, so in that case in dockerhub got to profile then security and create an access token so that you can login in to dockerhub through your terminal.
While building docker image after writing name after -t if you write :tag_name then it will gave a tag name to your docker image which you will see in dockerhub when you pusi it.


## Docker Compose
If you dont have docker compose in multi-container docker application then: 
- Create a network
"docker network create my_custom_network"
- Create a volume
"docker volume create volume_database"
- Start mongo container
"docker run -d -v volume_database:/data/db --name mongo --network my_custom_network  mongo"
- Start backend container
"docker run -d -p 3000:3000 --name backend --network my_custom_network backend"

Now what if you give a single command that will bootstrap everything. That would start the container, create volume, create network, everyting...
Docker Compose is a tool designed to help you define and run multi-container Docker applications. With Compose, you use a YAML file to configure your application's services, networks, and volumes. Then, with a single command, you can create and start all the services from your configuration.

- Install docker-compose - https://docs.docker.com/compose/install/
- Create a yaml file describing all your containers and volumes (by default all containers in a docker-compose run on the same network)
```yaml
    version: '3.8'
    services:
        mongodb:
            image: mongo
            container_name: mongodb
            ports:
                - "27017:27017"
            volumes:
                - mongodb_data:/data/db

    backend22:
        image: backend
        container_name: backend_app
        depends_on:
            - mongodb
        ports:
            - "3000:3000"
        environment:
            MONGO_URL: "mongodb://mongodb:27017"

    volumes:
        mongodb_data:
```
(click)[https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F085e8ad8-528e-47d7-8922-a23dc4016453%2F161f82ec-cbf1-4654-ab9b-a052fd1da6be%2FScreenshot_2024-03-10_at_5.36.58_PM.png?table=block&id=8e4f86ba-c720-4f78-8c97-1926391deb73&cache=v2]

- Start the compose
"docker-compose up"
- Stop everything (including volumes)
"docker-compose down --volumes"



## Docker Bind Mount
When creating a volume we have mount a folder /data/db to the volume. But lets say we want to bind a folder in the host machine i.e. our local pc to the folder in the container.
Now lets say we have made a container in which there is a next app. We made a dockerfile and build an image. Then we run that image in a container. Now when we change something in our code it wont reflect in the container running, as there wont be any hot reloading happening, obviousle, image is made with a copy of the directory.
Nextjs container we made has its own set of files. Changes made in local machine file wont reflect in the nextjs container file.

So when we make changes in the local machine code and make that changes reflect in the docker container, for that bind mounts is useful. It can take folder from your machine and bind that folder into the container's folder.

"docker run -p -v ./app:nextapp/app 3000:3000 nextapp"
./app:nextapp/app first written the folder dir from your pc and second written the folder dir to mount in the container's folder.
This is how we could give a real local developement experience to our end user because our end user need to have hot reloading, you cant expect your end user, they will start and then edit a file and stop, restart the docker container...