# S3 - Scalable Storge in the Cloud
Simple Storage Service
In this we have buckets to isolate/separate out data, in which we have objects (folder, video, images etc.). With an aws account we can make 100 buckets and in one bucket we can have unlimited data/objects.
So lets create a bucket. You can add objects lets say we add an image in the buvket. By default the public access of object is blocked. Now If we try to access it using the link then we wont be able to access. Although if we make the access public then too we wont be able to access the object. To access we have to add bucket policy to allow read public access to this bucket.
In policy we put Action to getObject and resource to this bucket name, eg. "arn:aws:s3:::varuntd-pencraft/*" 

In AWS, everything is an API call.

In a private bucket, noone can add/access any object. So after making a private bucket if we upload any static file(through console), we will be able to upload, because by clicking on upload button we are doing an api call from aws console/ui as a logged in user(lets say varun), so with the api call its sending a api call to upload that file on behalf of varun. Now if we try to open the link of the file, then its access denied. But if we click Open button from console, we can open it. There will be a query parameter in the Open button link thats security key. So its a presigned url because of which we are able to open that object. <br/>
So it made a temporary token using which we can were able to the object.

Now to do the same thing from nodejs project or somewhere else, We'll need a signature and token. Like on behalf of this user (lets say xyz) we are opening or uploading object(GET and PUT). We will need secretKey and accessKey. The IAM user with access to s3(if no access then the presigned access url made with the keys of that user wont open, as that user has no permission to bucket), can make these else root user can obviously make the signature and token.
So we make the IAM user and give that user the s3 permissions. And create acces key from their account, so using this user's keys we can access that private bucket objects.

Pre signed URL are for put object or for get object.

## Getting Object

(Read aws docs for latest info)
Now make a backend project and install aws sdk library in the project and a library for presigned url, something like @aws-sdk/s3-request-presigner.
Now we write like this
```js
import { S3Client } from "@aws-sdk/client-s3"

const s3Client = new S3Client({
    region: "ap-south-1",
    credentials: {
        accessKeyId: "",
        secretAccessKey: "",
    }
})

async function getObjectURL(key) {
    const command = new GetObjectCommand({
        Bucket: "varun-private",
        Key: key,
    });
    const url = await getSignedUrl(s3Client, command, { expiresIn: 20 });
    return url;
}
```
Put the keys we got from IAM user.


## Putting Object
Lets say we make a project, in which user uploads a video. What we can do is to get the video uploaded to server then give it to bucket, but it can increases the server cost very much, but what if video is big, what if many users upload big videos, what if server crashes... So thats not a good idea. Also we cant use this in serverless architecture as it has no memory. So we need to directly upload the object to the s3 directly. 

In this approach when we directly upload to the s3, we are exposing the logic, and secret keys to the frontend. Also how would we know that where to upload(which folder or file name in bucket).
The authorization and authentication happens at backend, but directly uploading to the bucket, means anyone can upload without auth check.

To tackle this problem, we have put objects presigned urls. User selects the video and frontend sends the video metadata to the backend and the backend will send the signed url with location as the bucket location where we want to put the video and type will be mp4, valid for lets say 1min.

```js
async function putObject(filename, contentType) {
    const command = new PutObjectCommand({
        Bucket: "varun-proivate",
        Key: `/uploads/user-uploads/${filename}`,
        ContentType: contentType,
    });
    const url = await getSignedUrl(s3Client, command);
    return url;
}
```
contentType eg: "image/jpeg"

now we call the putObject and we get the url. Now go to postman and send a put request with body as binary and select file and send. Now call the getObject and you get a url. Opening which you can find that video.


### List objects command
Lets say we want to list all the objects.

```js
async function listObjects() {
    const command = new ListObjectsV2Command({
        Bucket: "varun-private",
        Key: "/",
    });
    const result = await s3Client.send(command);
    console.log(result);
}
```

Similarly we can do many things, like delete and more...



Lets see more on S3
**Bucket Versioning**: So when you have a video, you overrides it with other video, which eventually increase the cost.

### AWS CloudTrail data events
Like employee working in office does have a log of what he's doing, similarly CloudTrail has track of what IAM user is doing what, it has the logs.

### Event notifications
Whenever lets say a event like put or delete or whatever we chose, triggers, then a lambda function we select will be executed.
